{"cells":[{"cell_type":"markdown","metadata":{"id":"f0dmmsG5NT7H"},"source":["# 1. Setting up PySpark\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"xQ7uModxiEeO","executionInfo":{"status":"ok","timestamp":1677514981437,"user_tz":-420,"elapsed":34210,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"outputs":[],"source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n","!tar xf spark-3.0.0-bin-hadoop3.2.tgz"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Mz57ADTokymR","executionInfo":{"status":"ok","timestamp":1677514981438,"user_tz":-420,"elapsed":14,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"outputs":[],"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"byWHkUycmKB2","executionInfo":{"status":"ok","timestamp":1677514985231,"user_tz":-420,"elapsed":3798,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"outputs":[],"source":["!pip install -q findspark\n","import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"aHIdq3_OmOyn","executionInfo":{"status":"ok","timestamp":1677514990990,"user_tz":-420,"elapsed":5763,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder\\\n","        .master(\"local\")\\\n","        .appName(\"Colab\")\\\n","        .config('spark.ui.port', '4050')\\\n","        .getOrCreate()\n","sc=spark.sparkContext"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1156,"status":"ok","timestamp":1677514992142,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"},"user_tz":-420},"id":"fOKZtyepm8l0","outputId":"220f5ab7-871b-4172-f537-6b02905866a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-27 16:23:10--  https://github.com/yoavfreund/UCSD_BigData_2016/raw/master/Data/Weather/stations_projections.pickle\n","Resolving github.com (github.com)... 140.82.113.3\n","Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/yoavfreund/UCSD_BigData_2016/master/Data/Weather/stations_projections.pickle [following]\n","--2023-02-27 16:23:10--  https://raw.githubusercontent.com/yoavfreund/UCSD_BigData_2016/master/Data/Weather/stations_projections.pickle\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2816937 (2.7M) [application/octet-stream]\n","Saving to: ‘/content/stations_projections.pickle’\n","\n","/content/stations_p 100%[===================>]   2.69M  --.-KB/s    in 0.01s   \n","\n","2023-02-27 16:23:11 (215 MB/s) - ‘/content/stations_projections.pickle’ saved [2816937/2816937]\n","\n"]}],"source":["!wget --continue https://github.com/yoavfreund/UCSD_BigData_2016/raw/master/Data/Weather/stations_projections.pickle -O /content/stations_projections.pickle"]},{"cell_type":"markdown","metadata":{"id":"VOgF-77eObYD"},"source":["# 2. Get weather dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677514992143,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"},"user_tz":-420},"id":"O8Ot0hWlOmRO"},"outputs":[],"source":["# library\n","# !pip install pathos\n","# from pathos.multiprocessing import ProcessingPool as Pool\n","\n","from multiprocessing.pool import ThreadPool as Pool\n","from functools import partial\n","import os\n","\n","import numpy as np \n","import pickle\n","import sys \n","import time\n","from numpy.linalg import norm \n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1614,"status":"ok","timestamp":1677514993753,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"},"user_tz":-420},"id":"tAqJq0AdmiFs","outputId":"ca149e9c-30ac-42c5-c844-24726eea44ab"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-f1b7be1ec272>:11: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n","  data = pickle.load(open(\"stations_projections.pickle\", \"rb\"), encoding='latin1')\n"]}],"source":["def parse_data(row):\n","    '''\n","    Parse each pandas row into a tuple of \n","    (station_name,  feature_vec),`l\n","    where feature_vec is the concatenation of the projection vectors\n","    of TAVG, TRANGE, and SNWD.\n","    '''\n","    return (row[0],\n","            np.concatenate([row[1], row[2], row[3]]))\n","## Read data\n","data = pickle.load(open(\"stations_projections.pickle\", \"rb\"), encoding='latin1')\n","rdd = sc.parallelize([parse_data(row[1]) \n","          for row in data.iterrows()])\\\n","        .map(lambda p: p[1]) # p = (name, vec)\n","                      "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1762,"status":"ok","timestamp":1677514995512,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"},"user_tz":-420},"id":"BvY6TnUzNP8L","outputId":"ef1bf3f3-8cde-419b-9dec-57e13f371562"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([ 3.04796236e+03,  1.97434852e+03,  1.50560792e+02, -2.90363288e+03,\n","        -2.36907268e+02,  1.47021791e+02,  1.91503001e-01,  1.87262808e-01,\n","        -4.01379553e-02]),\n"," array([ 2.07214900e+03,  8.80454659e+02, -1.94039657e+01, -1.58834407e+03,\n","         2.20915926e+01,  5.39057098e+01,  3.15437799e-01,  1.26292084e-01,\n","         7.92078997e-01])]"]},"metadata":{},"execution_count":8}],"source":["rdd.take(2)"]},{"cell_type":"markdown","metadata":{"id":"cOVZ2O3_PfNq"},"source":["# 3. Kmeans|| implementation"]},{"cell_type":"markdown","metadata":{"id":"cXflm5iUN9iE"},"source":["## 3.1 set up util functions and declare hyper-parameters"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"jZ2CGEfAS-7O","executionInfo":{"status":"ok","timestamp":1677514995513,"user_tz":-420,"elapsed":6,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"outputs":[],"source":["def print_log(s):\n","    '''\n","    Print progress logs\n","    '''\n","    sys.stdout.write(s + \"\\n\")\n","    sys.stdout.flush()\n","\n","def compute_entropy(d):\n","    '''\n","    Compute the entropy given the frequency vector `d`\n","    '''\n","    d = np.array(d)\n","    d = 1.0 * d / d.sum()\n","    return -np.sum(d * np.log2(d))\n","\n","def choice(p):\n","    '''\n","    Generates a random sample from [0, len(p)],\n","    where p[i] is the probability associated with i. \n","    '''\n","    random = np.random.random()\n","    r = 0.0\n","    for idx in range(len(p)):\n","        r = r + p[idx]\n","        if r > random:\n","            return idx\n","    assert(False)\n","\n","def samplePartition(index, pointCosts, total, l, seed):\n","    #############################################################\n","    # sample E(l) points from a partition\n","    #############################################################\n","    # first set a distinct state\n","    seed = seed ^ index\n","    np.random.RandomState(seed)\n","    samples = filter(lambda p: # p = point, cost\n","                        np.random.random()*total < l*p[1]\n","                  ,pointCosts)\n","    return map(lambda p: p[0], samples)\n","\n","def get_closest(p, centers):\n","    '''\n","    Return the indices the nearest centroids of `p`.\n","    `centers` contains sets of centroids\n","    '''\n","    best = 0\n","    closest = np.inf\n","    for j in range(len(centers)):\n","        temp_dist = norm(p - centers[j])\n","        if temp_dist < closest:\n","            closest = temp_dist\n","            best = j\n","    return best\n","\n","def get_loss(rdd, centers):\n","    costs = rdd.map(lambda p: norm(p-get_closest(p,centers), axis=0)**2) \n","    return costs.sum()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"WMrlT9G2PMS2","executionInfo":{"status":"ok","timestamp":1677514995513,"user_tz":-420,"elapsed":5,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"outputs":[],"source":["# Number of centroids\n","K = 5  \n","# For reproducability of results\n","RANDOM_SEED = 60295531 \n","# The K-means algorithm is terminated when the change in the \n","# location of the centroids is smaller than 0.1\n","converge_dist = 0.1"]},{"cell_type":"markdown","metadata":{"id":"zZnwidS8OJG1"},"source":["## 3.2 MapReduce k-means++ for benchmark"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"skNAbLv6PcKS","executionInfo":{"status":"ok","timestamp":1677514995514,"user_tz":-420,"elapsed":6,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"outputs":[],"source":["def kmeans_pp_init(rdd, K, seed):\n","    #######################################################\n","    # parallelize k-means++\n","    #######################################################\n","    local_data = rdd.collect()\n","    def update_cost(vec, center, dist):\n","        return np.min([dist, norm(vec - center, axis=0)**2], axis=0)\n","    # cost vector\n","    costs = rdd.map(lambda _: np.inf)\n","\n","    # Randomly select the first point for every run of k-means++\n","    newCenter = rdd.takeSample(False, 1, seed)[0]\n","    centers = [newCenter]\n","\n","    for idx in range(K - 1):\n","        ########################################################\n","        # In each iteration, you need to select one point. \n","        # For each data point x, let D(x) be the\n","        # distance between x and the nearest center. Choose a new\n","        # data point using a weighted probability proportional\n","        # to D(x)^2\n","        ########################################################\n","        bcNewCenter = sc.broadcast(newCenter)\n","        # Update cost\n","        costs = (rdd.zip(costs)\n","                    .map(lambda p: #p = (point, cost)\n","                                update_cost(p[0], bcNewCenter.value, p[1]))\n","                    .persist())\n","        # compute sum(D(x)^2)\n","        total = costs.sum()\n","        bcNewCenter.unpersist()\n","        # Normalize each distance to get the probabilities\n","        prob = costs.map(lambda p: p/total).collect()\n","\n","        newCenter = local_data[choice(prob)]\n","        centers += [newCenter]\n","    return centers"]},{"cell_type":"markdown","metadata":{"id":"VFU3wqEzOYJw"},"source":["## 3.3 k-means|| a.k.a kmeans_ss_init implementation"]},{"cell_type":"markdown","metadata":{"id":"8K5X0ID0OmKT"},"source":["### a) local k-means++ implementation "]},{"cell_type":"code","execution_count":12,"metadata":{"id":"3B85PjGCBmIj","executionInfo":{"status":"ok","timestamp":1677514995514,"user_tz":-420,"elapsed":6,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"outputs":[],"source":["def _kmeans_pp_init(data, K, seed, w=None):\n","    ##########################################################\n","    # run k-means++ locally\n","    ##########################################################\n","    # sample the first center based on weight \n","    n_data = len(data)\n","    if w is None: # uniformize weight vector if not exist\n","        w = [1]*n_data\n","        sample = np.random.randint(n_data)\n","    else: # pick sample based on distribution vector w\n","        sample = np.random.choice(range(n_data), p=[k/np.sum(w) for k in w])\n","\n","    costs = [np.inf]*n_data\n","    # the `centers` variable is what we want to return\n","    centers = np.zeros((K, data[0].shape[0]))\n","    # Randomly select the first point for every run of k-means++\n","    centers[0] = data[sample]\n","\n","    def update_cost(vec, dist, k):\n","        new_dist = norm(vec - centers[k], axis=0)**2\n","        return np.min([dist, new_dist], axis=0)\n","    for idx in range(K - 1):\n","        # Update cost D(x)^2\n","        costs = [update_cost(point, cost, idx) for point, cost in zip(data, costs)]\n","        # Calculate sum of w(x)*D(x)^2\n","        total = np.sum(np.multiply(w, costs))\n","        # Normalize each distance to get the probabilities\n","        prob = np.divide([weight*cost for weight, cost in zip(w, costs)], total)\n","        #K'th centroid for each run\n","        centers[idx+1] = data[choice(prob)]\n","    return centers"]},{"cell_type":"markdown","metadata":{"id":"4_n1Wr5wOsQK"},"source":["### b) MapReduce k-means|| implementation"]},{"cell_type":"code","source":["# %%file utill.py\n","# from numpy.linalg import norm \n","# def cost_p(center, point): \n","#     return norm(point-center, axis=0)**2"],"metadata":{"id":"BbR_IKzD47ZW","executionInfo":{"status":"ok","timestamp":1677514995515,"user_tz":-420,"elapsed":6,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"iVhSKwtut-0L","executionInfo":{"status":"ok","timestamp":1677514995515,"user_tz":-420,"elapsed":6,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"outputs":[],"source":["# from utill import cost_p\n","def pointCost(centers, point):\n","    # with Pool(processes = os.cpu_count()) as pool:\n","    #     partial_dist = partial(cost_p, point=point)\n","    #     dist = pool.map(partial_dist, centers)\n","    #     print(dist)\n","    # return dist\n","    return [norm(point - center, axis=0)**2 for center in centers] \n","\n","def kmeans_ss_init(rdd, K, seed):\n","    #############################################################\n","    # overseeding l = 5K, t = 5  chosen according to Bachem.2017\n","    #############################################################\n","    l = 2*K # number of points in each iteration\n","    t = 5 # number of round\n","    # Initialize empty centers and point squared distance.\n","    costs = rdd.map(lambda _: np.inf)\n","\n","    # Initialize the first center to a random point.\n","    newCenters = rdd.takeSample(False, 1, seed)\n","    centers = newCenters\n","\n","    for idx in range(t):\n","        bcNewCenters = sc.broadcast(newCenters)\n","        # update cost vector again new centers\n","        costs = (rdd.zip(costs)\n","                    .map(lambda p: #p = (point, cost)\n","                       np.min(pointCost(bcNewCenters.value, p[0])+[p[1]]))\n","                    .persist())\n","        # compute sum(D(x)^2)\n","        total = costs.sum()\n","        bcNewCenters.unpersist()\n","\n","        # sample E(l) new centers\n","        newCenters = (rdd.zip(costs)\n","                         .mapPartitionsWithIndex(\n","                              lambda index, pointCosts: \n","                              samplePartition(index, pointCosts, total, l, seed^(idx<<16))\n","                              )\n","                         .collect())\n","        centers += newCenters\n","    #############################################################\n","    # Reclustering l*t+1 points into K points\n","    #############################################################\n","    costs.unpersist()\n","    distCenters = np.unique(centers, axis=0)\n","    bcCenters = sc.broadcast(distCenters) \n","    # count number of points within clusters\n","    countMap = (rdd.map(lambda p: # p = point\n","                        get_closest(p, bcCenters.value))\n","                   .countByValue())\n","    bcCenters.destroy()\n","    # compute the weight vector\n","    weights = [countMap.get(i, 0) for i in range(len(distCenters))] \n","    # locally perform kmeans++ on the shrinked dataset\n","    return _kmeans_pp_init(distCenters, K, seed, weights)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wXUshMKBO3VN"},"source":["## 3.4 MapReduce k-means Lloyd iteration implementation"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"7w9Sdv3PXhLL","executionInfo":{"status":"ok","timestamp":1677514995515,"user_tz":-420,"elapsed":6,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}}},"outputs":[],"source":["def kmeans_assign(rdd, k_points):\n","    label = (rdd\n","              .map(lambda p: # p = point\n","                  (get_closest(p, k_points), \n","                    (p, 1))) \n","              .reduceByKey(lambda x,y: \n","                            np.sum([x, y], axis=0))\n","              .map(lambda p:  # p = (i, (sum(Si), |Si|))\n","                  (p[0], p[1][0]/p[1][1]))\n","              .collect())\n","    return dict(label)\n","\n","\n","def kmeans(rdd, K, converge_dist, seed, init_method):\n","    '''\n","    Run K-means++ algorithm on `rdd`, where `RUNS` is the number of\n","    initial sets to use.\n","    '''\n","    k_points = init_method(rdd, K, seed)\n","    print_log(\"Initialized.\")\n","    temp_dist = 1.0\n","\n","    iters = 0\n","    st = time.time()\n","    while temp_dist > converge_dist: \n","        ###############################################################\n","        # Update all `RUNS` sets of centroids using standard k-means \n","        ###############################################################\n","        # assign new centers\n","        new_points = kmeans_assign(rdd, k_points)\n","\n","        # shift in centers\n","        temp_dist = np.max([\n","                np.sum([norm(k_points[j] - new_points[j]) \n","                for j in new_points.keys()])\n","                            ])\n","\n","        iters = iters + 1\n","        if iters % 5 == 0:\n","            print_log(\"Iteration %d max shift: %.2f (time: %.2f)\" %\n","                      (iters, temp_dist, time.time() - st))\n","            st = time.time()\n","\n","        # update old centroids\n","        # You modify this for-loop to meet your need\n","        for (j, p) in new_points.items():\n","            k_points[j] = p\n","\n","    return k_points"]},{"cell_type":"markdown","metadata":{"id":"5C-2UkSpPEy7"},"source":["## 3.5 Benchmark"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35609,"status":"ok","timestamp":1677515031118,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"},"user_tz":-420},"id":"y4LHx9IbkyPX","outputId":"2e793a21-5176-4b56-8708-9c649f5b52e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized.\n","Iteration 5 max shift: 1923.19 (time: 6.99)\n","Iteration 10 max shift: 221.77 (time: 3.04)\n","Iteration 15 max shift: 118.59 (time: 3.97)\n","Iteration 20 max shift: 53.96 (time: 2.97)\n","Iteration 25 max shift: 43.26 (time: 2.55)\n","Iteration 30 max shift: 43.03 (time: 2.59)\n","Iteration 35 max shift: 8.19 (time: 2.45)\n","Iteration 40 max shift: 3.65 (time: 3.94)\n","Loss =  237752343938.56964\n","Time takes parallel k-means++ to converge: 35.75950884819031\n"]}],"source":["# bench mark parallel k-means++\n","st = time.time()\n","\n","np.random.seed(RANDOM_SEED)\n","centroids = kmeans(rdd, K, converge_dist, \n","                   np.random.randint(1000), kmeans_pp_init)\n","print(\"Loss = \", get_loss(rdd, centroids))\n","# group = rdd.mapValues(lambda p: get_closest(p, centroids)) \\\n","#            .collect()\n","\n","print(\"Time takes parallel k-means++ to converge:\", time.time() - st)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mkVvtaa1t_cq","executionInfo":{"status":"ok","timestamp":1677515052940,"user_tz":-420,"elapsed":21836,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}},"outputId":"d1af7a8c-f5c6-4dc8-b973-3171fc33a560"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized.\n","Iteration 5 max shift: 481.24 (time: 3.53)\n","Iteration 10 max shift: 211.39 (time: 2.51)\n","Iteration 15 max shift: 40.73 (time: 2.46)\n","Iteration 20 max shift: 7.39 (time: 2.77)\n","Loss =  237717607151.1062\n","Time takes parallel k-means|| to converge: 21.654953002929688\n"]}],"source":["# bench mark parallel k-means||\n","st = time.time()\n","\n","np.random.seed(RANDOM_SEED)\n","centroids = kmeans(rdd, K, converge_dist, \n","                  np.random.randint(1000), kmeans_ss_init)\n","print(\"Loss = \", get_loss(rdd, centroids))\n","\n","# group = rdd.mapValues(lambda p: get_closest(p, centroids)) \\\n","#            .collect()\n","\n","print(\"Time takes parallel k-means|| to converge:\", time.time() - st)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"LYfpmRvkha1j","executionInfo":{"status":"ok","timestamp":1677515084304,"user_tz":-420,"elapsed":31378,"user":{"displayName":"Duy Khánh","userId":"18432659802611150428"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d66f174-47f7-48d6-8812-d1c3a05993bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized.\n","Iteration 5 max shift: 1333.51 (time: 2.84)\n","Iteration 10 max shift: 1432.02 (time: 2.56)\n","Iteration 15 max shift: 792.09 (time: 2.80)\n","Iteration 20 max shift: 570.63 (time: 2.39)\n","Iteration 25 max shift: 119.48 (time: 3.57)\n","Iteration 30 max shift: 62.97 (time: 2.43)\n","Iteration 35 max shift: 39.63 (time: 2.85)\n","Iteration 40 max shift: 37.14 (time: 2.46)\n","Iteration 45 max shift: 10.41 (time: 2.93)\n","Iteration 50 max shift: 3.53 (time: 3.17)\n","Iteration 55 max shift: 0.00 (time: 2.86)\n","Loss =  237706475377.01685\n","Time takes local k-means++ to converge: 31.54694437980652\n"]}],"source":["# bench mark random k-means\n","def kmeans_rand_init(rdd, K, seed):\n","    return rdd.takeSample(False, K, seed)\n","st = time.time()\n","\n","np.random.seed(RANDOM_SEED)\n","centroids = kmeans(rdd, K, converge_dist, \n","                   np.random.randint(1000), kmeans_rand_init)\n","print(\"Loss = \", get_loss(rdd, centroids))\n","\n","# group = rdd.mapValues(lambda p: get_closest(p, centroids)) \\\n","#            .collect()\n","\n","print(\"Time takes local k-means++ to converge:\", time.time() - st)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOQkJKXxmow7X6nNInLaB8+"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}